{"cells":[{"cell_type":"markdown","source":["# Задание:\n","### Используйте ниже приведенные ссылки на PDF файлы вакансии и резюме на позицию HR Директора.\n","### 1) Сформируйте 10 \"Ключевых требований к кандидату\" на основании вакансии для последующего анализа (нужно сформировать промпты, с указанием вакансии, отправить запрос к OpenAI вывести и сохранить ответ).\n","### 2) На основании сгенерированных \"Ключевых требований к кандидату\", текста вакансии и текста резюме Критично оцените, насколько кандидат подходит на указанную вакансию (нужно сформировать промпты, с указанием вакансии, резюме и Ключевых требовании к вакансии, отправить запрос к OpenAI и вывести ответ)."],"metadata":{"id":"78SM20xTyKlL"}},{"cell_type":"code","source":["#@title Ссылки на файлы вакансии и резюме на позицию HR Директора\n","\n","# Файл резюме\n","resume_link = 'https://disk.yandex.ru/i/ULV_Onsdrx7zrw'\n","\n","# Файл вакансии\n","vacancy_link = 'https://disk.yandex.ru/i/Lg7ZhWiutBVpXg'"],"metadata":{"id":"Dk1Y7N_Mo1Tv","executionInfo":{"status":"ok","timestamp":1759925157956,"user_tz":-180,"elapsed":17,"user":{"displayName":"летучая мыш нет","userId":"15606522630716566801"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#@title Установка необходимых библиотек\n","!pip install -q openai langchain langchain_community \\\n","langchain_openai langchain_core faiss-cpu \\\n","rarfile PyPDF2\n"],"metadata":{"id":"KZ2vIXyYi-Es","executionInfo":{"status":"ok","timestamp":1759925224519,"user_tz":-180,"elapsed":4824,"user":{"displayName":"летучая мыш нет","userId":"15606522630716566801"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#@title Импорт библиотек, активация ключей, папки, глобальные переменные\n","\n","from langchain.text_splitter import MarkdownHeaderTextSplitter\n","from langchain_core.output_parsers import JsonOutputParser\n","from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n","from langchain_core.pydantic_v1 import BaseModel, Field\n","from langchain_community.vectorstores import FAISS\n","from langchain.docstore.document import Document\n","from langchain.prompts import PromptTemplate\n","\n","from pydub import AudioSegment\n","from io import BytesIO\n","import aiohttp\n","from google.colab import userdata, drive\n","from PyPDF2 import PdfReader\n","from IPython.display import Audio, display, clear_output\n","from datetime import datetime\n","import requests\n","from urllib.parse import urlencode\n","import textwrap\n","import rarfile\n","import openai\n","import gdown\n","import json\n","import time\n","import os\n","import re\n","\n","# Подключаем ГуглДиск для работы с файлами\n","drive.mount('/content/drive')\n","# Формируем основной путь\n","path = '/content/drive/MyDrive/data/hr/'\n","# Формирование дополнительных путей\n","path_vacancies_pdf = f'{path}vacancies_pdf/'\n","path_vacancies_json = f'{path}vacancies_json/'\n","path_resumes_pdf = f'{path}resumes_pdf/'\n","path_resumes_json = f'{path}resumeses_json/'\n","path_db_faiss = f'{path}db_faiss/'\n","path_add_data = f'{path}add_data/'\n","\n","folders = [path, path_vacancies_pdf, path_vacancies_json, path_resumes_pdf,\n","           path_resumes_json, path_db_faiss, path_add_data ]\n","# Создание необходимых папок на ГуглДиске: '/data/hr/...'\n","for folder in folders:\n","    os.makedirs(folder, exist_ok=True)\n","\n","# Cоздание или очистка файла логов, если файл уже есть\n","open(os.path.join(path, '_log.txt'), 'w').close()"],"metadata":{"id":"L8c1fZubi9xv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759925219691,"user_tz":-180,"elapsed":40091,"user":{"displayName":"летучая мыш нет","userId":"15606522630716566801"}},"outputId":"12d8a425-f331-454a-a32a-cdcf024c1620"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n","\n","For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n","with: `from pydantic import BaseModel`\n","or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n","\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n","  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n","  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n","  elif re.match('(flt)p?( \\(default\\))?$', token):\n","/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n","  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["api_key = \"sk-or-vv-78b1c1e8c37d56f2f654ce34bfb5dc3bc8eebc7c103a6e7cbde4816f881376eb\" # ваш ключ в VseGPT после регистрации\n","base_url = \"https://api.vsegpt.ru/v1\"\n","\n","# Проверка наличия API ключа (лучше делать это в начале программы)\n","if not api_key:\n","    raise ValueError(\"Необходимо указать ключ API.  Используйте переменную окружения OPENAI_API_KEY или укажите ключ напрямую (не рекомендуется для продакшена).\")\n","\n","# --- Настройка модели ---\n","model_config = {  # Используем словарь для конфигурации модели\n","    \"title\": \"o4-mini (vsegpt)\",\n","    \"provider\": \"openai\",\n","    \"model\": \"openai/o4-mini\",\n","    \"apiBase\": base_url,\n","    \"apiType\": \"openai\",\n","    \"apiKey\": api_key,  # Используем полученный API ключ\n","    \"useLegacyCompletionsEndpoint\": False\n","}\n","\n","\n","# Настройка клиента OpenAI (Инициализируем клиент один раз)\n","client = openai.OpenAI(\n","    api_key=model_config[\"apiKey\"],\n","    base_url=model_config[\"apiBase\"],\n",")\n","\n","'''openai.api_key = model_config[\"apiKey\"]\n","openai.api_base = model_config[\"apiBase\"]'''"],"metadata":{"id":"_Ithmk7Y8KzW","executionInfo":{"status":"ok","timestamp":1759925219697,"user_tz":-180,"elapsed":3,"user":{"displayName":"летучая мыш нет","userId":"15606522630716566801"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","# Функция для форматирования текста по абзацам\n","def format_text(text, width=120):\n","    # Разделяем текст на абзацы\n","    paragraphs = str(text).split('\\n')\n","    # Форматируем каждый абзац отдельно\n","    formatted_paragraphs = []\n","    for paragraph in paragraphs:\n","        # Используем textwrap.fill для форматирования абзаца, чтобы длина строки не превышала width\n","        formatted_paragraph = textwrap.fill(paragraph, width)\n","        formatted_paragraphs.append(formatted_paragraph)\n","    # Объединяем абзацы с символом новой строки\n","    return '\\n'.join(formatted_paragraphs)\n","\n","\n","# Функция записи логов в файл _log.txt\n","def add_log_file(text, title=''):\n","    time_now = f\"{datetime.now().strftime('%Y-%m-%d %H:%M')}\"\n","    with open(os.path.join(path, '_log.txt'), \"a\", encoding='utf-8') as file:\n","            file.write(f'\\n\\n{time_now}. {title}.\\n\\n{format_text(text)}')\n","\n","\n","\n","# Функция генерации ответа от OpenAI\n","def generate_answer(prompt_system, prompt_user, prompt_assistant='', model='openai/o4-mini', temp=1):\n","    messages = [\n","        {\"role\": \"system\", \"content\": prompt_system},\n","        {'role': 'assistant', 'content': prompt_assistant},\n","        {\"role\": \"user\", \"content\": prompt_user}\n","    ]\n","    response = openai.chat.completions.create(\n","        model=model,\n","        messages=messages,\n","        temperature=temp,\n","        # max_tokens=4000\n","    )\n","    # Запись в лог-файл\n","    add_log_file(f'\\n{messages}\\n{response.choices[0].message.content}\\n\\n\\n', title='generate_answer()')\n","    return response.choices[0].message.content\n","\n","\n","\n","# Формируем поля для парсига Вакансии для последующего использовании при поиске\n","# https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/json/\n","\"\"\"Класс Vacancy с использованием библиотеки Pydantic, который будет использоваться для парсинга\n","   вакансий. Класс имеет четыре поля: position, skills, requirements и responsibilities. Описание\n","   для каждого поля включает инструкцию по извлечению соответствующих данных из текста вакансии.\"\"\"\n","add_prompt = \"Иначе, выведи ответ: None\"\n","class Vacancy(BaseModel):\n","    position: str = Field(  # Добавлено \"= Field(\"\n","        description = f'Найди в тексте вакансии название должности (позиции). {add_prompt}')\n","    skills: str = Field( # Добавлено \"= Field(\"\n","        description = re.sub(r'\\s+', ' ', f\"\"\"Найди в тексте вакансии все указанные навыки.\n","                      {add_prompt}\"\"\"))\n","    requiments: str = Field( # Добавлено \"= Field(\"\n","        description = re.sub(r'\\s+', ' ', f\"\"\"Найди в тексте вакансии все указанные требования.\n","                      {add_prompt}\"\"\"))\n","    resposibilities: str = Field( # Добавлено \"= Field(\"\n","        description = re.sub(r'\\s+', ' ', f\"\"\"Найди в тексте вакансии все указанные обязанности.\n","                      {add_prompt}\"\"\"))\n","\n","# Формируем поля для парсига Резюме для последующего использовании при поиске\n","# https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/json/\n","\"\"\"Класс Resume с использованием библиотеки Pydantic, который будет использоваться для парсинга\n","   резюме. Класс имеет три поля: position, skills и experience. Описание для каждого поля включает\n","   инструкцию по извлечению соответствующих данных из текста вакансии.\"\"\"\n","\n","add_prompt = \"Иначе, выведи ответ: None\"\n","class Resume(BaseModel):\n","    position: str = Field(\n","        description = f'Найди в тексте резюме название должности (позиции). {add_prompt}')\n","    skills: str = Field(\n","        description = re.sub(r'\\s+', ' ', f\"\"\"Найди в тексте резюме все указанные навыки (стек).\n","                      {add_prompt}\"\"\"))\n","    experience: str = Field(\n","        description = re.sub(r'\\s+', ' ', f\"\"\"Найди в тексте резюме описание прошлого опыта с\n","                      описанием деятельности. {add_prompt}\"\"\"))\n","\n","\n","# Парсинг текста с формированием полей по параметрам (parser_class=Vacancy или Resume)\n","# JSON parser https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/json/\n","def to_dict_parser(text, parser_class, model='openai/o4-mini') -> dict:\n","    # text: текст для парсинга\n","    # parser_class: класс парсера (например, Vacancy или Resume)\n","    # model: имя модели для генерации текста (по умолчанию 'openai/o4-mini')\n","\n","    # Экземпляр JsonOutputParser (из langchain)\n","    parser = JsonOutputParser(pydantic_object=parser_class)\n","    prompt = PromptTemplate(\n","        input_variables = [\"query\"],\n","        # шаблон с инструкциями\n","        template = \"Follow the instructions:\\n\"\n","                   \"{format_instructions}\\n\"\n","                   \"{query}\\n\",\n","        # встроенные инструкции JsonOutputParser по форматированию\n","        partial_variables = {\"format_instructions\": parser.get_format_instructions()})\n","\n","    # цепочка (chain) из шаблона, модели и парсера\n","    # оператор '|' означает последовательное выполнение: шаблон -> модель -> парсер\n","    model = ChatOpenAI(openai_api_key=model_config[\"apiKey\"], openai_api_base=model_config[\"apiBase\"])\n","    chain = prompt | model | parser\n","    return chain.invoke({\"query\": text})\n","\n","\n","# Парсинг PDF файлов Вакансий и подготовка чанков для векторизации\n","# Cохранение данных вакансий в Json для удобства последующего использования\n","def pdf_to_json_and_chunks_vacancies(path_vacancies_pdf, model='openai/o4-mini-mini'):\n","    chunks_vacancies = [] # список для хранения чанков\n","    for file in os.listdir(path_vacancies_pdf): # Перебираем все файлы в указанной директории\n","        if file.endswith('.pdf'): # является ли файл PDF\n","            # From PDF to text\n","            vacancy = read_pdf(os.path.join(path_vacancies_pdf, file))\n","            add_log_file(vacancy, title='vacancy')\n","            # Парсинг текста вакансии и преобразование его в словарь с использованием заданного парсера и модели\n","            dict_vacancy = to_dict_parser(vacancy, parser_class=Vacancy, model=model)\n","            add_log_file(dict_vacancy, title='vacancy_to_dict_parser')\n","\n","            vacancy_id = file.split('.')[0] # Имя файла без расширения .pdf\n","            # записываем в словарь дополнительные поля\n","            dict_vacancy['id'] = vacancy_id\n","            dict_vacancy['vacancy'] = vacancy # в т.ч. Вакансию целиком в словарь\n","\n","            # Сохрание всех данных словаря в Json файл\n","            with open(os.path.join(path_vacancies_json, f'{vacancy_id}.json'), 'w', encoding='utf-8') as f:\n","                json.dump(dict_vacancy, f, ensure_ascii=False)\n","\n","            # Формирование чанка для векторной базы вакансий\n","            chunk = re.sub(r'\\s+', ' ',\n","                        f\"\"\"1.Позиция: {dict_vacancy.get('position', '')}.\n","                            2.Навыки: {dict_vacancy.get('skills', '')}.\n","                            3.Требования: {dict_vacancy.get('requirements', '')}.\n","                            4.Обязанности: {dict_vacancy.get('responsibilities', '')}.\"\"\")\n","            # Добавляем чанк каждой вакансии в список в формате Langchain Document\n","            chunks_vacancies.append(Document(page_content=chunk,\n","                                    metadata={\"meta\": f\"{vacancy_id}\"}))\n","    add_log_file(chunks_vacancies, title='pdf_to_json_and_chunks_vacancies')\n","    return chunks_vacancies # список чанков вакансий\n","\n","\n","# Парсинг PDF файлов Резюме и подготовка чанков для векторизации\n","# Cохранение данных резюме в Json для удобства последующего использования\n","def pdf_to_json_and_chunks_resumes(path_resumes_pdf, model='openai/o4-mini'):\n","    chunks_resumes = [] # список для хранения чанков\n","    for file in os.listdir(path_resumes_pdf): # Перебираем все файлы в указанной директории\n","        if file.endswith('.pdf'): # является ли файл PDF\n","            # From PDF to text\n","            resume = read_pdf(os.path.join(path_resumes_pdf, file))\n","            add_log_file(resume, title='read_pdf_resume')\n","            # Парсинг текста резюме и преобразование его в словарь с использованием заданного парсера и модели\n","            dict_resume = to_dict_parser(resume, parser_class=Resume, model=model)\n","            add_log_file(dict_resume, title='resume_to_dict_parser')\n","\n","            resume_id = file.split('.')[0] # Имя файла резюме без расширения .pdf\n","            # записываем в словарь дополнительные поля\n","            dict_resume['id'] = resume_id\n","            dict_resume['resume'] = resume # в т.ч. Резюме целиком в словарь\n","\n","            # Сохрание всех данных словаря в Json файл\n","            with open(os.path.join(path_resumes_json, f'{resume_id}.json'), 'w', encoding='utf-8') as f:\n","                json.dump(dict_resume, f, ensure_ascii=False)\n","\n","            # Формирование чанка для векторной базы резюме\n","            chunk = re.sub(r'\\s+', ' ',\n","                        f\"\"\"1.Позиция: {dict_resume.get('position', '')}.\n","                            2.Навыки: {dict_resume.get('skills', '')}.\n","                            3.Опыт: {dict_resume.get('experience', '')}.\"\"\")\n","            # Добавляем чанк каждого резюме в список в формате Langchain Document\n","            chunks_resumes.append(Document(page_content=chunk,\n","                                  metadata={\"meta\": f\"{resume_id}\"}))\n","    add_log_file(chunks_resumes, title='pdf_to_json_and_chunks_resumes')\n","    return chunks_resumes # список чанков\n","\n","    # Поиск релевантных чанков по запросу\n","def scores_and_meta_similarity(query, db_index, k=3):\n","    # query: текст для поиска релевантой информации\n","    # db_index: векторная база данных\n","    # k: количество возвращаемых результатов (по умолчанию 3)\n","\n","    # Нахождение наиболее похожих документов на основе заданного запроса\n","    docs_and_scores = db_index.similarity_search_with_score(query, k=k)\n","    add_log_file(docs_and_scores, title='scores_and_meta_similarity')\n","    return ([doc[1] for doc in docs_and_scores],                   # List оценок (score) по результатам поиска\n","            [doc[0].metadata['meta'] for doc in docs_and_scores])  # Мeta list с именами файлов\n","\n","\n","# Загрузка фала из YandexDisk\n","def download_from_yandex_disk(link, output_path):\n","    # link: ссылка на файл в Yandex Диске\n","    # output_path: путь, куда будет сохранен загруженный файл\n","    # Базовый URL для получения информации о ресурсе\n","    base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources?'\n","    # Формируем финальный URL для запроса информации о ресурс\n","    final_url = base_url + urlencode(dict(public_key = link))\n","    # Делаем запрос для получения информации о ресурсе\n","    response = requests.get(final_url)\n","    resource_info = response.json()\n","    # Получаем ссылку на скачивание файла\n","    download_url = resource_info['file']\n","    # Получаем имя файла\n","    file_name = resource_info['name']\n","    # Скачиваем файл(response - переводится, как ответ)\n","    download_response = requests.get(download_url)\n","    with open (os.path.join(output_path, file_name), 'wb') as f:\n","      f.write(download_response.content)\n","    add_log_file(link, title=\"Yandex_link\") # Запись в лог-файл\n","\n","\n","# Разархивирование содержимого архива в ту же папку\n","def unrar(path):\n","    for rar in os.listdir(path): # пробегаем по всем файлам в папке\n","        if rar.endswith('.rar'): # является ли файл арховом RAR\n","            filepath = os.path.join(path, rar) # путь к RAR файлу\n","            with rarfile.RarFile(filepath) as opened_rar:\n","                opened_rar.extractall(path) # разархивирование в ту же папку\n","\n","# Функция дублирования строки с символом '#...'\n","def duplicate_lines(text):\n","    lines = text.split('\\n')\n","    result = []\n","    for line in lines:\n","        if line.startswith('#'): # строка с символом '#...'\n","            result.append(line)\n","        result.append(line.lstrip('# ')) # дублируем без '#'\n","    return '\\n'.join(result)\n","\n","\n","# Создание векторной базы из текстового файла с Markdown разметкой\n","def db_from_markdown_file(markdown_file):\n","    # Открываем и читаем содержимое Markdown файла\n","    with open(markdown_file, 'r', encoding='utf-8') as f:\n","        markdown_info = f.read()\n","    markdown_info = duplicate_lines(markdown_info)\n","    # Определяем заголовки, по которым будет выполняться разбиение текста\n","    # В данном случае, это заголовки уровня 1 (#)\n","    headers_to_split_on = [(\"#\", \"Header 1\"),]\n","    # Экземпляр MarkdownHeaderTextSplitter с указанными заголовками для разбиения текста\n","    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n","    # Разбиваем содержимое Markdown файла на чанки на основе заголовков\n","    chunks = splitter.split_text(markdown_info)\n","    # Создаем и возвращаем векторную базу данных FAISS\n","    return FAISS.from_documents(chunks, OpenAIEmbeddings(openai_api_key=model_config[\"apiKey\"], openai_api_base = model_config[\"apiBase\"]))\n","\n","\n","  # Функция диалога, поочередной подачи вопросов из списка и получение ответов\n","def ask_questions(questions):\n","    responses = []\n","    for question in questions:\n","        print(format_text(f\"Рекрутер: {question}\"), '\\n')\n","        answer = input(\"Кандидат: \")\n","        formatted_answer = format_text(f\"Кандидат: {answer}\")\n","        # добаляем историю диалога в список\n","        responses.append(f\"{question}\\n\\n{formatted_answer}\\n\\n\")\n","        print('\\n')\n","    add_log_file(responses, title='ask_questions(questions)')\n","    return responses\n","\n","\n","    # Функция типовых вопросов\n","def conduct_interview(resume):\n","    text_start = f'Рекрутер: Здравствуйте! Давайте начнем собеседование. План собеседования следующий: \\n 1. Я задам Вам несколько вопросов. \\n 2. Расскажу о нашей компании и имеющейся вакансии \\n 3. Отвечу на Ваши вопросы. \\n Общайтесь пожалуйста со мной как с обычным рекрутером!'\n","    print(text_start, '\\n')\n","    interview_summary = [f\"Резюме:\\n{resume}\\n\", \"Интервью:\\n\"]\n","    # добаляем историю диалога в список\n","    interview_summary.extend(ask_questions(questions))\n","    add_log_file(\"\\n\".join(interview_summary), title='conduct_interview')\n","    # возвращаем резюме и историю диалога в текстовом виде (str)\n","    return \"\\n\".join(interview_summary)\n","\n","\n","# Функция анализа диалога и генерация доп. вопросов\n","def generate_dialogue(prompt_system, prompt_user, model='openai/o4-mini'):\n","    print(f'Рекрутер: Подождите минутку. Я проанализирую Ваши ответы и сформулирую дополнительные вопросы.', '\\n')\n","    answer = generate_answer(prompt_system, prompt_user, model=model)\n","    print(format_text(answer))\n","    return answer\n","\n","# Функция дополнительных вопросов\n","def ask_additional_questions(questions):\n","    question_list = questions.split('\\n') # формируем список из строк\n","    question_list = [q for q in question_list if q.strip()] # убираем из списка пустые строки\n","    additional_responses = ask_questions(question_list) # функция диалога\n","    add_log_file(additional_responses, title='ask_additional_questions') # Запись в лог-файл\n","    return additional_responses\n","\n","\n","# Функция презентации компании\n","def present_company_and_vacancy(company_description):\n","    text_presentation = f'Рекрутер: Спасибо, что ответили на все дополнительные вопросы. Теперь я хочу подробнее рассказать Вам о нашей организации. Нажмите Enter, когда будете готовы.'\n","    print(format_text(text_presentation), '\\n')\n","    input()\n","    print(format_text(company_description), '\\n\\n')\n","\n","# Функция генерации оценки кандидата\n","def generate_estimation(prompt_system, prompt_user, model='openai/o4-mini-mini'):\n","    print(\"Итоговый отчет по кандидату:\", '\\n')\n","    answer = generate_answer(prompt_system, prompt_user, model=model)\n","    print(format_text(answer))\n","    return answer"],"metadata":{"id":"EzZfCzlDk0Li","executionInfo":{"status":"ok","timestamp":1759926454177,"user_tz":-180,"elapsed":187,"user":{"displayName":"летучая мыш нет","userId":"15606522630716566801"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# Решение"],"metadata":{"id":"2bl3ZiJarjxG"}},{"cell_type":"code","source":["#@title\n","\n","def read_pdf(pdf_file):\n","    try:\n","        reader = PdfReader(pdf_file)\n","        # возвращаем текст построчно\n","        return ' '.join([line.extract_text() for page in reader.pages for line in page.extract_text().splitlines() if line])\n","    except Exception as e:\n","        print(f\"Ошибка при чтении PDF файла: {e}\")\n","        return None"],"metadata":{"id":"CSUvj-qJqpue","executionInfo":{"status":"ok","timestamp":1759926365872,"user_tz":-180,"elapsed":7,"user":{"displayName":"летучая мыш нет","userId":"15606522630716566801"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["#@title Загрузка PDF файлов вакансий и резюме\n","\n","# Загрузка PDF файлов вакансий в RAR архиве\n","rar_vacancies = 'https://disk.yandex.ru/i/Lg7ZhWiutBVpXg'\n","download_from_yandex_disk(rar_vacancies, path_vacancies_pdf)\n","unrar(path_vacancies_pdf) # разархивирование PDF вакансий в ту же папку\n","\n","# Загрузка PDF файлов резюме в RAR архиве\n","rar_resumes = 'https://disk.yandex.ru/i/ULV_Onsdrx7zrw'\n","download_from_yandex_disk(rar_resumes, path_resumes_pdf)\n","unrar(path_resumes_pdf) # разархивирование PDF резюме в ту же папку"],"metadata":{"id":"u9kquwiqJsZ5","executionInfo":{"status":"ok","timestamp":1759926377141,"user_tz":-180,"elapsed":9856,"user":{"displayName":"летучая мыш нет","userId":"15606522630716566801"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["#@title Парсинг PDF, создания векторных баз\n","\n","# Парсинг PDF файлов вакансий и резюме, формирование чанков\n","chunks_vacancies = pdf_to_json_and_chunks_vacancies(path_vacancies_pdf)\n","chunks_resumes = pdf_to_json_and_chunks_resumes(path_resumes_pdf)\n","\n","# Создание и сохранение векторной базы для вакансий\n","embeddings = OpenAIEmbeddings(\n","    openai_api_key=model_config[\"apiKey\"],\n","    openai_api_base=model_config[\"apiBase\"]\n",")\n","\n","db_vacancies = FAISS.from_documents(chunks_vacancies, embeddings)\n","db_vacancies.save_local(folder_path=path_db_faiss, index_name='db_vacancies')\n","\n","# Создание и сохранение векторной базы для резюме\n","db_resumes = FAISS.from_documents(chunks_resumes, embeddings)\n","db_resumes.save_local(folder_path=path_db_faiss, index_name='db_resumes')\n","\n","print(\"Векторные базы успешно созданы и сохранены.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6cd5_LDdTbDE","executionInfo":{"status":"ok","timestamp":1759926495513,"user_tz":-180,"elapsed":32717,"user":{"displayName":"летучая мыш нет","userId":"15606522630716566801"}},"outputId":"7768af0e-31fb-4e63-92e2-b08aeddaa255"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Ошибка при чтении PDF файла: EOF marker not found\n","Ошибка при чтении PDF файла: 'str' object has no attribute 'extract_text'\n","Ошибка при чтении PDF файла: 'str' object has no attribute 'extract_text'\n","Ошибка при чтении PDF файла: 'str' object has no attribute 'extract_text'\n","Ошибка при чтении PDF файла: EOF marker not found\n","Ошибка при чтении PDF файла: 'str' object has no attribute 'extract_text'\n","Векторные базы успешно созданы и сохранены.\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"colab":{"provenance":[{"file_id":"1t1CqJA2gx9ad6ZjEYxHsCiXH8IUigtkl","timestamp":1759852182501},{"file_id":"1YCvL0fi9VerygSM6SA_DQ_H6EJ-008ad","timestamp":1759573727983}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}